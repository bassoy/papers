\section{Experimental Setup}
\label{sec:experimental.setup}

\begin{figure*}[t]
\begin{footnotesize}Â¸
\begin{alignat*}{3}
\mbN_1 &=
\begin{bmatrix}
(16384,1024 )          & \cdots & (131072, 1024) \\
(8192,1024, 2)         & \cdots & (65536, 1024, 2) \\
\vdots                 & \ddots & \vdots \\
(64,1024, 2,\dots,2)   & \cdots & (512, 1024, 2,\dots,2) \\
\end{bmatrix}
\quad
\quad
&\mbN_{10} &= &&
\begin{bmatrix}
(1024, 16384)          & \cdots & (1024, 131072) \\
(1024, 2,8192)         & \cdots & (1024,2,65536) \\
\vdots                 & \ddots & \vdots \\
(1024, 2,\dots,2,64)   & \cdots & (1024,2,\dots,2, 512) \\
\end{bmatrix}\\[0.5cm]
\mbM &=
\begin{bmatrix}
(4096,4096)           & \cdots & (8192, 8192) \\
(256,256,256)         & \cdots & (512, 512, 512) \\
(64,64,64,64)         & \cdots & (128, 128, 128,128) \\
(32,32,32,32,32)      & \cdots & (64,  64,  64, 64, 64) \\
(16,16,16,16,16,16)   & \cdots & (32,  32,  32, 32, 32, 32) \\
(8, 8, 8,  8, 8, 8,8) & \cdots & (16,  16,  16, 16, 16, 16, 16) \\
\end{bmatrix}
\quad
\quad
&\mbQ &=&& 
%utl::Range {utl::Dim{ 26,1800,3600    },  utl::Dim{0,0,1  },   utl::Dim{ 26,1800,3600    }},   // CESM-ATM
%utl::Range {utl::Dim{100, 500, 500, 13},  utl::Dim{0,0,0,1},   utl::Dim{100, 500, 500, 13}},   // Hurricane ISABEL
%utl::Range {utl::Dim{512, 512, 512,  6},  utl::Dim{0,0,0,1},   utl::Dim{512, 512, 512,  6}},   // NYX
%utl::Range {utl::Dim{ 98,1200,1200, 13},  utl::Dim{0,0,0,1},   utl::Dim{ 98,1200,1200, 13}},   // SCALE-LETKF        
%utl::Range {utl::Dim{ 69,  69, 115,288},  utl::Dim{0,0,0,1},   utl::Dim{ 69,  69, 115,288}},   // QMCPACK
%utl::Range {utl::Dim{256, 384, 384,  7},  utl::Dim{0,0,0,1},   utl::Dim{256, 384, 384,  7}},   // Miranda
%utl::Range {utl::Dim{500, 500, 500, 11},  utl::Dim{0,0,0,1},   utl::Dim{500, 500, 500, 11}},   // S3D
%utl::Range {utl::Dim{986,  32, 185,388},  utl::Dim{0,0,0,1},   utl::Dim{986,  32, 185,388}},   // EXAFEL
\begin{bmatrix}
(26,1800,3600)\\
(100, 500, 500, 13)\\
(512, 512, 512,  6)\\
(98,1200,1200, 13) \\
(69,  69, 115,288)\\
(256, 384, 384,  7)\\
(500, 500, 500, 11)\\
(986,  32, 185,388)
\end{bmatrix}
\end{alignat*}
\end{footnotesize}
\caption[Shape Matrix]
{
Four shape tuple matrices $\mbN_1$, and $\mbN_{10}$ that have been used for the runtime measurements.
Each shape creates asymmetrically-shaped tensor or subtensor. 
}
\label{fig:tensor.shapes}
\end{figure*}
\subsection{Computing System} 
The experiments have been carried out on a dual socket Intel Xeon Gold 5318Y CPU with an Ice Lake architecture and a dual socket AMD EPYC 9354 CPU with a Zen4 architecture.
With two NUMA domains, the Intel CPU consists of $2\times24$ cores which run at a base frequency of $2.1$ GHz.
Assuming a peak AVX-512 Turbo frequency of $2.5$ GHz, the CPU is able to process $3.84$ TFLOPS in double precision.
%$2.5$ GHz x $2$ FMA units / cores x $16$ OPS / FMA unit x $48$ cores = $3.84$ Tflops.
We measured a peak double-precision floating-point performance of $3.8043$ TFLOPS ($79.25$ GFLOPS/core) and a peak memory throughput of $288.68$ GB/s using the Likwid performance tool.
%3328
The AMD EPYC 9354 CPU consists of $2\times32$ cores running at a base frequency of $3.25$ GHz.
Assuming an all-core boost frequency of $3.75$ GHz, the CPU is theoretically capable of performing $3.84$ TFLOPS in double precision.
%$3.25$ GHz x $1$ FMA units / cores x $16$ OPS / FMA unit x $64$ cores = $3.84$ Tflops.
We measured a peak double-precision floating-point performance of $3.87$ TFLOPS ($60.5$ GFLOPS/core) and a peak memory throughput of $788.71$ GB/s.
 
We have used the GNU compiler v11.2.0 with the highest optimization level \ttt{-O3} together with the \ttt{-fopenmp} and \ttt{-std=c++17} flags. 
Loops within the eighth case have been parallelized using GCC's OpenMP v4.5 implementation.
In case of the Intel CPU, the 2022 Intel Math Kernel Library (MKL) and its threading library \ttt{mkl\_intel\_thread} together with the threading runtime library \ttt{libiomp5} has been used for the three BLAS functions \ttt{gemv}, \ttt{gemm} and \ttt{gemm\_batch}.
For the AMD CPU, we have compiled AMD AOCL v4.2.0 together with set the \ttt{zen4} architecture configuration option and enabled OpenMP threading.


\subsection{OpenMP Parallelization}
The loops in the \ttt{par-loop} algorithms have been parallelized using the OpenMP directive \ttt{omp parallel for} together with the \ttt{schedule(static)}, \ttt{num\_threads(ncores)} and \ttt{proc\_bind}\allowbreak\ttt{(spread)} clauses.
In case of tensor-slices, the \ttt{collapse(2)} clause has been added for transforming both loops into one loop which has an iteration space of the first loop times the second one. 
We also had to enable nested parallelism using \ttt{omp\_set\_nested} to toggle between single- and multi-threaded \ttt{gemm} calls for different TTM cases when using AMD AOCL.

The \ttt{num\_threads(ncores)} clause specifies the number of threads within a team where \ttt{ncores} is equal to the number of processor cores. 
Hence, each OpenMP thread is responsible for computing $\bar{n}' / \text{\ttt{ncores}}$ independent slice-matrix products where $\bar{n}' = n_2' \cdot n_4'$ for tensor slices and $\bar{n}' = n_4'$ for mode-$\mhq$ subtensors.

The \ttt{schedule(static)} instructs the OpenMP runtime to divide the iteration space into equally sized chunks, except for the last chunk.
Each thread sequentially computes $ \bar{n}' / \text{\ttt{ncores}}$ slice-matrix products.
We have decided to use this scheduling kind as all slice-matrix multiplications exhibit the same number of floating-point operations with a regular workload where one can assume negligible load imbalance.
Moreover, we wanted to prevent scheduling overheads for small slice-matrix products were data locality can be an important factor for achieving higher throughput.

The \ttt{OMP\_PLACES} environment variable has not been explicitly set and thus defaults to the OpenMP \ttt{cores} setting which defines an OpenMP place as a single processor core. % 1 place <-> 1 core
Together with the clause \ttt{num\_threads(ncores)}, the number of OpenMP threads is equal to the number of OpenMP places, i.e. to the number of processor cores.
We did not measure any performance improvements for a higher thread count.

The \ttt{proc\_bind(spread)} clause additionally binds each OpenMP thread to one OpenMP place which lowers inter-node or inter-socket communication and improves local memory access.
Moreover, with the \ttt{spread} thread affinity policy, consecutive OpenMP threads are spread across OpenMP places which can be beneficial if the user decides to set \ttt{ncores} smaller than the number of processor cores.

 

\subsection{Tensor Shapes} 
We have evaluated the performance of our algorithms with asymmetrically and symmetrically shaped tensors to account for a wide range of use cases.
Their corresponding tensor shapes are divided into three tensor sets $N$, $M$ and $Q$.
Examples of the corresponding tensor shape arrays $\mbN_1$ and $\mbN_{10}$, $\mbM$ and $\mbQ$ are shown in Fig. \ref{fig:tensor.shapes}.

Tensors of the first set $N$ are created with $720 = 9\times 8 \times 10$ different tensor shapes which are organized within ten two-dimensional shape arrays $\mbN_k$ of size $9 \times 8$ with $k=1,\dots,10$.
A dimension tuple $\mbn_{r,c}^k$ is of length $r+1$ with $r = 1,\dots,9$ and $c = 1,\dots,8$.
With $i = 1,\dots,r+1$ The $i$th element of the tuple is either $1024$ for $i = 1 \wedge k \neq 1$ or $i = 2 \wedge k = 1$, or $c \cdot 2^{15-r}$ for $i = \min(r+1,k)$ or $2$ otherwise.
For each tensor shape matrix $\mbN_k$, our benchmark suite generates input tensors with their respective shapes and an input matrix with dimensions $n_1$ or $n_2$ and $n_q$, and performs a mode-$k$ TTM.
A special feature of this test set is that the contraction dimension and the leading dimension are disproportionately large.

%This set covers $10$ contraction modes ranging from $1$ to $10$.
%For each contraction mode, the tensor order increases from $2$ to $10$ and for a given tensor order, $8$ tensor instances with increasing tensor size are generated.
The second tensor set $M$ consists of symmetrically shaped tensors which are generated with $48$ tensor shapes which are arranged within a two-dimensional shape array $\mbM$ of size $6 \times 8$.
Similar to the previous setup, the row number $r$ is equal to the tensor order $r+1$ with $r=1,\dots,6$
A row of the tensor shape array consists $8$ dimension tuples of the same length $r+1$ where elements of one dimension tuple are equal.
With eight shapes and the step size of each row $s_r = (m_{r,8}-m_{r,1})/8$, the respective intermediate dimensions $m_{r,c}$ are given by $m_{r,c} = m_{r,1} + (c-1) s_r$ with $c=1,\dots,8$.
Our benchmark suite generates input tensors with their respective shapes and an input matrix with dimensions $n_1$ or $n_0$ and $n_q$, and performs a mode-$q$ TTM for all possible contraction modes $q$ with $q=1,\dots,7$.
A similar setup has been used in \cite{bassoy:2019:ttv, bassoy:2018:fast}.

%artifical tensors 
We have also benchmarked TTM implementations with eight tensors that are part of the scientific data reduction benchmark (SDRBench) \cite{zhao:2020:sdrbench}.
The scientific datasets in SDRBench mainly consist of order-$3$ tensors with different tensor shapes and number of data fields, originating from various types real-world applications and simulations such as CESM-ATM or Miranda.
We perform runtime tests with order-$4$ tensors which belong to the tensor set $Q$.
Their first three dimensions correspond to the respective ones mentioned in the original data sets and the last dimension to the number of data fields.
The respective tensor shapes are arranged within a one-dimensional array $\mbQ$, see Fig. \ref{fig:tensor.shapes}.
Starting from the top first tensor shape of $\mbQ$, the respective tensor dimensions have been used in CESM-ATM, Hurricane-ISABEL, NYX, SCALE-LETKF, QMCPACK, Miranda, S3D and EXAFEL.



%They describe 10+ scientific datasets across 6+ domains to support a fair, comprehensive assessment of lossy compressors. 
%All the datasets are provided with information including description, shape, data type, and data size. 
%In addition, some datasets are provided with physical information of the application and specific user requirements on compression errors (such as absolute error bounds and point-wise relative error bounds).
%Each of the corresponding simulations/applications may potentially produce extremely large volume of data.

%Hence, CESM-ATM, ISABEL and SCALE-LETKF, from cosmology simulation NYX
%The datasets are as follows:
%The first data set consists of order-3 tensors with different number of data fields that originate from climate simulation projects called CESM-ATM, ISABEL and SCALE-LETKF.
%The second data set consists of an order-3 tensor NYX, originates from cosmology simulation.

%We consider two large-scale simulation datasets which were produced by S3D [9], a massively parallel direct numerical simulation of compressible reacting flows, developed at Sandia National Laboratories. 



%Community Earth System Model (CESM). CESM-ATM involves 60+ snapshots(timesteps), each containing 100+ fields with different dimensions.
%The second dataset, Hurricane-ISABEL, is from IEEE Visualization 2004 contest [27]. 
%The dataset simulates the ISABEL hurricane - the strongest hurricane in the 2003 Atlantic hurricane season. 
%The dataset contains 13 floating-point fields in single-precision, and each field is a 3D array with the shape of 100 Ã 500 Ã 500.


