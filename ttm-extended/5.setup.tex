\section{Experimental Setup}
\label{sec:experimental.setup}

\begin{table*}[t]
\input{tab.dataset}
\caption%
{%
\footnotesize
Tensor shape sets and example dimension tuples that are used in our runtime benchmarking.
The first $4$ shape sets $N_1$, $N_2$, $N_3$ and $N_{10}$ are used to generate asymmetrically shaped tensors, each consisting of $72$ dimension tuples.
Shape set $M$ contains $48$ tensor shapes that are used to generate symmetrically shaped tensors.
Shape set $Q$ contains $8$ tensor shapes that are part of SDRBench \cite{zhao:2020:sdrbench}.
Note that all matrix shapes depend on the input tensor shapes and contraction mode.
}
\label{tab:dataset}
\end{table*}



\subsection{Computing System} 
The runtime benchmark have been executed on a dual socket Intel Xeon Gold 5318Y CPU with an Ice Lake architecture and a dual socket AMD EPYC 9354 CPU with a Zen4 architecture.
With two NUMA domains, the Intel CPU consists of $2\times24$ cores which run at a base frequency of $2.1$ GHz.
Assuming a peak AVX-512 Turbo frequency of $2.5$ GHz, the CPU is able to process $3.84$ TFLOPS in double precision.
%$2.5$ GHz x $2$ FMA units / cores x $16$ OPS / FMA unit x $48$ cores = $3.84$ Tflops.
We have measured a peak double-precision floating-point performance of $3.8043$ TFLOPS ($79.25$ GFLOPS/core) and a peak memory throughput of $288.68$ GB/s using the Likwid performance tool.
%3328
The AMD EPYC 9354 CPU consists of $2\times32$ cores running at a base frequency of $3.25$ GHz.
Assuming an all-core boost frequency of $3.75$ GHz, the CPU is theoretically capable of performing $3.84$ TFLOPS in double precision.
%$3.25$ GHz x $1$ FMA units / cores x $16$ OPS / FMA unit x $64$ cores = $3.84$ Tflops.
We measured a peak double-precision floating-point performance of $3.87$ TFLOPS ($60.5$ GFLOPS/core) and a peak memory throughput of $788.71$ GB/s.
 
All libraries have been compiled with the GNU compiler v11.2.0 using the highest optimization level \ttt{-O3} together with the \ttt{-fopenmp} and \ttt{-std=c++17} flags. 
Loops within the eighth case have been parallelized using GCC's OpenMP v4.5 implementation.
In case of the Intel CPU, the Intel Math Kernel Library 2022 (MKL) and its threading library \ttt{mkl\_intel\_thread}, threading runtime library \ttt{libiomp5} has been used for the three BLAS functions \ttt{gemv}, \ttt{gemm} and \ttt{gemm\_batch}.
For the AMD CPU, the AMD library AOCL v4.2.0 has been compiled with the \ttt{zen4} flag.


\subsection{OpenMP Parallelization}
The loops in the \ttt{par-loop} algorithms have been parallelized using the OpenMP directive \ttt{omp parallel for} together with the \ttt{schedule(static)}, \ttt{num\_threads(ncores)} and \ttt{proc\_bind}\allowbreak\ttt{(spread)} clauses.
In case of tensor-slices, the \ttt{collapse(2)} clause has been added for transforming both loops into one loop which has an iteration space of the first loop times the second one. 
We also had to enable nested parallelism using \ttt{omp\_set\_nested} to toggle between single- and multi-threaded \ttt{gemm} calls for different TTM cases when using AMD AOCL.

The \ttt{num\_threads(ncores)} clause specifies the number of threads within a team where \ttt{ncores} is equal to the number of processor cores. 
Hence, each OpenMP thread is responsible for computing $\bar{n}' / \text{\ttt{ncores}}$ independent slice-matrix products where $\bar{n}' = n_2' \cdot n_4'$ for tensor slices and $\bar{n}' = n_4'$ for mode-$\mhq$ subtensors.

The \ttt{schedule(static)} instructs the OpenMP runtime to divide the iteration space into equally sized chunks, except for the last chunk.
Each thread sequentially computes $ \bar{n}' / \text{\ttt{ncores}}$ slice-matrix products.
We have decided to use this scheduling kind as all slice-matrix multiplications exhibit the same number of floating-point operations with a regular workload where one can assume negligible load imbalance.
Moreover, we wanted to prevent scheduling overheads for small slice-matrix products were data locality can be an important factor for achieving higher throughput.

The \ttt{OMP\_PLACES} environment variable has not been explicitly set and thus defaults to the OpenMP \ttt{cores} setting which defines an OpenMP place as a single processor core. % 1 place <-> 1 core
Together with the clause \ttt{num\_threads(ncores)}, the number of OpenMP threads is equal to the number of OpenMP places, i.e. to the number of processor cores.
We did not measure any performance improvements for a higher thread count.

The \ttt{proc\_bind(spread)} clause additionally binds each OpenMP thread to one OpenMP place which lowers inter-node or inter-socket communication and improves local memory access.
Moreover, with the \ttt{spread} thread affinity policy, consecutive OpenMP threads are spread across OpenMP places which can be beneficial if the user decides to set \ttt{ncores} smaller than the number of processor cores.

 
\subsection{Data sets} 
The runtime performance of our algorithms have been executed with asymmetrically and symmetrically shaped tensors to account for a wide range of use cases.
The corresponding tensor shapes are divided into $12$ sets $N_1$, $N_2$, \dots, $N_{10}$, $M$ and $Q$.
Table \ref{tab:dataset} contains example dimension tuples for the input tensor and matrix.
The shape of the latter is $(n_2,n_q)$ if $q=1$ and $(n_1,n_q)$ otherwise where $q$ is the contraction mode with $1 \leq q \leq p$.
The computation of the output tensor dimensions is described in Section~\ref{subsec:ttm}.

The first ten shape sets $N_1$ to $N_{10}$ contain $9 \times 8$ tensor shapes all of which generate asymmetrically shaped tensors.
Within one set $N_k$, dimension tuples are arranged within $10$ two-dimensional shape arrays $\mbN_k$ of size $9 \times 8$ with $1 \leq k \leq 10$.
A dimension tuple $\mbn_{r,c}$ within $\mbN_k$ is of length $r+1$ with $1 \leq r \leq 9$ and $1 \leq c \leq 8$.
%With $1 \leq i \leq r+1$, 
Its $i$-th element is either $1024$ for $i = 1 \wedge k \neq 1$ or $i = 2 \wedge k = 1$, or $c \cdot 2^{15-r}$ for $i = \min(r+1,k)$ or $2$ otherwise.
A special feature of this test set is that the contraction dimension and the leading dimension are disproportionately large.

%This set covers $10$ contraction modes ranging from $1$ to $10$.
%For each contraction mode, the tensor order increases from $2$ to $10$ and for a given tensor order, $8$ tensor instances with increasing tensor size are generated.
The second shape set $M$ contains $48$ tensor shapes that used to generate symmetrically shaped tensors.
The shapes are arranged within one two-dimensional shape array $\mbM$ of size $6 \times 8$.
Similar to the previous setup, the row number $r$ is equal to the tensor order $r+1$ with $1 \leq r 6$.
A row of the tensor shape array consists $8$ dimension tuples of the same length $r+1$ where elements of one dimension tuple are equal such that $m_{r,c} = \mbm_{r,c}(i) = \mbm_{r,c}(j)$ for $1 \leq i,j \leq r+1$.
With eight shapes and the step size of each row $s_r = (m_{r,8}-m_{r,1})/8$, the respective intermediate dimensions $m_{r,c}$ are given by $m_{r,c} = m_{r,1} + (c-1) s_r$ with $1 \leq c \leq 8$.
Symmetrically and asymmetrically shaped tensors have also been used in \cite{bassoy:2019:ttv, bassoy:2018:fast}.

%artifical tensors 
We have also benchmarked our algorithm using eight tensors from the scientific data reduction benchmark (SDRBench) \cite{zhao:2020:sdrbench}.
The scientific datasets in SDRBench mainly consist of order-$3$ tensors with different tensor shapes and number of data fields, originating from various real-world simulations.
Tensors from the SP dataset for instance has been used for benchmarking the truncated Tucker decomposition in \cite{ballard:2020:tuckermpi}
We perform runtime tests with order-$4$ tensors that are generated with dimension tuples of the tensor shape set $Q$.
Their first three dimensions correspond to the respective ones mentioned in the original data sets and the last dimension to the number of data fields.
All tensor shapes are provided in Table \ref{tab:dataset}.


\subsection{Profiling setup} 
Our benchmark suite iterates through dimension tuples from one of tensor shape sets for one contraction mode $q$ with $1 \leq q \leq \max_p$ where $\max_p$ is the maximum tensor order within the shape set.
Tensor and matrix elements are randomly generated single-precision floating-point numbers in case of the data set $Q$.
In all other test cases double-precision is used.
The profiler first sweeps through tensor shapes belonging to one tensor order and then iteratively selects one larger tensor order for the next sweep.
Given a dimension tuple, the profiler generates two tensors and a matrix, executes a mode-$q$ TTM implementation $20$ times and finally computes the median runtime of the benchmarked TTM implementation.
To prevent caching of the output tensor, we invalidate caches which is excluded from the timing.
 
The runtime results for one contraction mode and one TTM implementation are stored in a two-dimensional array with shape $\max_p \times k$ where $k$ is either $8$ in case of asymmetrically and symmetrically shaped tensors or $1$ in case of the set $Q$.
It should be noted that if $q>p$, the contraction mode $q$ is set to $p$.
Hence, our profiler generates $10$ runtime arrays of shape $9\times 8$ with asymmetrically shaped tensors for $10$ contraction modes using the shape sets $N_1$, $N_2$, \dots, $N_{10}$.
Generating symmetrically shaped tensors with the shape set $M$, the profiler returns $7$ runtime arrays of shape $6 \times 8$ for $7$ contraction modes.
Using the shape set $Q$, $4$ one-dimensional runtime arrays for $4$ contraction modes are computed.

The three-dimensional runtime data generated with the data sets $N$ and $M$ can be used to create two dimensional performance maps, as it is done in the following Section~\ref{sec:results}.
Each value in a performance map corresponds to a mean or median value over tensor sizes (i.e. dimension tuples with the same length), over tensor orders or contraction modes.
