\begin{abstract}
%
The tensor-matrix multiplication is a basic tensor operation required by various tensor methods such as the HOSVD.
%
This paper presents flexible high-performance algorithms that compute the tensor-matrix product according to the Loops-over-GEMM (LoG) approach.
%
Our algorithms are able to process dense tensors with any linear tensor layout, arbitrary tensor order and dimensions all of which can be runtime variable.
%
We discuss two slicing methods with orthogonal parallelization strategies and propose four algorithms that call BLAS with subtensors or tensor slices.
%
We provide a simple heuristic which selects one of the four proposed algorithms at runtime.
%
All algorithms have been evaluated on a large set of tensors with various tensor shapes and linear tensor layouts.
%
In case of large tensor slices, our best-performing algorithm achieves a median performance of $2.47$ TFLOPS on an Intel Xeon Gold 5318Y and $2.93$ TFLOPS an AMD EPYC 9354.
%
Furthermore, it outperforms batched GEMM implementation of Intel MKL by a factor of $2.57$ with large tensor slices.
%
Our runtime tests show that TLIB'S function \ttt{<combined>} is, in median, between $15.38$\% and $257.58$\% faster than most state-of-the-art approaches, including actively developed libraries like Libtorch and Eigen.
%
It is on par with TBLIS for many tensor shapes which uses optimized kernels for the TTM computation.
%
This work is an extended version of the article "Fast and Layout-Oblivious Tensor-Matrix Multiplication with BLAS" (Bassoy, 2024)\cite{bassoy:2024:ttm}.
\end{abstract}
