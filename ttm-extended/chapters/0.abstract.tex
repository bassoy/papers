\begin{abstract}
%
The tensor-matrix multiplication is a basic tensor operation required by various tensor methods such as the HOSVD.
%
This paper presents flexible high-performance algorithms that compute the tensor-matrix product according to the Loops-over-GEMM (LoG) approach.
%
Our algorithms are able to process dense tensors with any linear tensor layout, arbitrary tensor order and dimensions all of which can be runtime variable.
%
We discuss two slicing methods with orthogonal parallelization strategies and propose four algorithms that call BLAS with subtensors or tensor slices.
%
We provide a simple heuristic which selects one of the four proposed algorithms at runtime.
%
All algorithms have been evaluated on a large set of tensors with various tensor shapes and linear tensor layouts.
%
In case of large tensor slices, our best-performing algorithm achieves a median performance of $2.47$ TFLOPS on an Intel Xeon Gold 5318Y and $2.93$ TFLOPS an AMD EPYC 9354.
%
Furthermore, it outperforms batched GEMM implementation of Intel MKL by a factor of $2.57$ with large tensor slices.
%
For the majority of our test tensors, our implementation is on average $25.05$\% faster than other state-of-the-art approaches, including actively developed libraries like Libtorch and Eigen.
\end{abstract}
