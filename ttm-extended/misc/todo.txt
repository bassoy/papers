todo [before submission]:

[x] reformulate the test setup - processors, libraries, max flops, MKL / AOCL
[x] integrate OpenMP as perhaps implemetnation details? (first general  ) 
[x] rename *optimized* to *combined* 
[x] rename slice-qd to order-q subtensor 
[x] rename *cases* to *TTM cases*
[x] explain omp_set_nested into the OpenMP section
[x] update the introduction/contribution
[x] update the conclusion
[x] refer to the original paper and reference it 
[] read your paper
[] go through the TODOs in the paper
[x] remove \tf formatting in the titles of some TLIB figures 
[] adjust line breaking and bad formatting

todo [after submission]:

[] merge ttm back to master
[x] rename `optimized` to `combined` 


todo [review1]:

Major concerns:
[] This paper is not self-contained, e.g., lines 228 and 632. 
    [x] rework line 228: the algorithm description has been updated and is now self-contained. it has been made clear that the following algorithm design is similar to one that has been previously described. 
    [] 

Please write important things in this paper and do not refer to other papers to omit the explanation. Otherwise, readers will have to open the referred papers to read this paper.
[] The author mentions that the performance of matrix multiplication is not memory-bound in line 156, but it is not always true, for example, a multiplication of two tall-skinny matrices and bached small matrix multiplications. For better performance analysis, it would be better to use a roofline model in the evaluation section.
[] What is the essential difference between Algorithm 2 and TTGT? Is it true that both are the same if we put all \underbar{A'}_{i,j} matrices in a continuous memory space and interpret it as a large matrix?
[] There are some undefined words, such as TTV and TTM.
[] It would be better to write an explanation of the experimental evaluation in a summary way. It is very hard to understand the evaluation results from the explanations. Also, there are many detailed numbers, especially on page 11, which make it difficult to understand the summary of the evaluation. What about changing the vertical axis of Figure 2 to the performance efficiency?
[] Are there any tensor shapes where the proposed method has lower performance than the existing methods? The cumulative axis does not help to show this.

Minor concerns:
[] It would be better to make a table or something to explain , etc, and also not to use the same label for subtensors and tensor slices, for example, in Figure 2.
[] Line 226: tensor-times-matrix -> tensor-matrix
[] What is the "see [17, 14]" in line 204 for?
[] Please cite more papers if the author write "many", for example, in lines 2 and 32.
