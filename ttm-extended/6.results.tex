\section{Results and Discussion}
\label{sec:results}

\begin{comment}
\begin{table*}[t]
\caption%
{%
\footnotesize
Combinable and non-combinable function tags for the eighth TTM case.
%Function tag \tf{combined} and \tf{gemm-batch} are not combinable with the other tags.
}
\centering
\footnotesize
\begin{tabular}{ll}% 
\toprule
Tag & Explanation \\
\midrule
\tf{<par-gemm> }    & \tf{ttm} sequentially executes multi-threaded gemm (section 4.4.1) \\
\tf{<par-loop> }    & \tf{ttm} executes single-threaded \tf{gemm}s in parallel (section 4.4.2) \\
\tf{<slice>}        & \tf{ttm} executes single- or multi-threaded gemm operates with tensor slices \\
\tf{<subtensor>}    & \tf{ttm} executes single- or multi-threaded gemm operates with subtensors  \\  
\tf{<combined>}     & \tf{ttm} calls one of the above functions w.r.t processor core count and tensor dimensions (section 4.4.3) \\
\tf{<batched-gemm>} & \tf{ttm} calls function \tf{gemm\_batch} function of the MKL (section 4.4.4) \\
\bottomrule
\end{tabular}
%\vspace{0.2cm}
\label{tab:function_tags}
\end{table*}
\end{comment}


\begin{figure*}[t]
\input{fig.performance.tlib.contour-mkl-parloop}
\input{fig.performance.tlib.contour-mkl-pargemm} 
\caption{%
\footnotesize %
Performance contour plots in double-precision GFLOPS/core of the proposed TTM algorithms \tf{<par-loop>} and \tf{<par-gemm>} with varying tensor orders $p$ and contraction modes $q$. 
The top row of maps (1x) depict measurements of the \tf{<par-loop>} versions while the bottom row of maps with number (2x) contain measurements of the \tf{<par-gemm>} versions.
Tensors are asymmetrically shaped on the left four maps (a,b) and symmetrically shaped on the right four maps (c,d).
Tensor $\mubA$ and $\mubC$ have the first-order while matrix $\mbB$ has the row-major ordering.
All functions have been measured on an Intel Xeon Gold 5318Y.
\label{fig:performance.tlib.contour}
}
\end{figure*}


\begin{figure*}[t]
\input{fig.performance.tlib.case8-cdf-mkl}
\input{fig.performance.tlib.case8-cdf-aocl}
\caption{ %
\footnotesize%
Cumulative performance distributions in double-precision GFLOPS/core of the proposed algorithms for the eighth case.
Each distribution belongs to one algorithm:
\tf{<batched-gemm>} (\ref{coord:gemm_batch}), %
\tf{<combined>} (\ref{coord:optimized}),
\tf{<par-gemm>,slice>} (\ref{coord:seq_loops_par_gemm_slice}) and
\tf{<par-loop,slice>} (\ref{coord:par_loops_seq_gemm_slice}),
\tf{<par-gemm,subtensor>} (\ref{coord:seq_loops_par_gemm_subtensor}) and
\tf{<par-loop,subtensor>} (\ref{coord:par_loops_seq_gemm_subtensor}).
The top row of maps (1x) depict measurements performed on an Intel Xeon Gold 5318Y with the MKL while the bottom row of maps with number (2x) contain measurements performed on an AMD EPYC 9354 with the AOCL.
Tensors are asymmetrically shaped in (a) and (b) and symmetrically shaped in (c) and (d).
Input matrix has the row-major ordering (rm) in (a) and (c) and column-major ordering (cm) in (b) and (d).
}
\label{fig:performance.tlib.case8}
\end{figure*}


\begin{figure*}[t]
\input{fig.performance.tlib.format.mkl}
\input{fig.performance.tlib.format.aocl}
%\input{fig.performance.tlib.format}
\caption{ %
\footnotesize%
Box plots visualizing performance statics in double-precision GFLOPS/core of the function with row-major (left) or column-major matrices (right).
Box plot number $k$ denotes the $k$-order tensor layout of symmetrically shaped tensors with order $7$.
% where the $1$-order and $7$-order layouts are the first- and last-order storage formats, respectively.
}
\label{fig:performance.tlib.format}
\end{figure*}

\begin{figure*}[t]
\input{fig.performance.comparison-mkl}
\input{fig.performance.comparison-aocl} 
\caption{ %
\footnotesize%
Cumulative performance distributions of TTM implementations in double-precision GFLOPS/core.
Each distribution corresponds to a library:
\textbf{TLIB}[ours] (\ref{coord:nonsymmetric.tlib.slice}), %
\textbf{TCL} (\ref{coord:nonsymmetric.tcl}), %
\textbf{TBLIS} (\ref{coord:nonsymmetric.tblis}), %
\textbf{LibTorch} (\ref{coord:nonsymmetric.libtorch}), %
\textbf{Eigen} (\ref{coord:nonsymmetric.eigen}).
Libraries have been tested with asymmetrically-shaped (left plot) and symmetrically-shaped tensors (right plot).
}
\label{fig:performance.comparison}
\end{figure*}






\subsection{Slicing Methods}
This section analyzes the performance of the two proposed slicing methods \ttt{<slice>} and \ttt{<subtensor>} that have been discussed in section \ref{subsec:parallel.multi-loops}.
%Note that this analysis is equal to minimizing or maximizing the input parameter $M_C$.
Fig. \ref{fig:performance.tlib.contour} contains eight performance contour plots of four \ttt{ttm} functions \ttt{<par-loop>} and \ttt{<par-gemm>}.
Both functinos either compute the slice-matrix product with subtensors \ttt{<subtensor>} or tensor slices \ttt{<slice>} on the Intel Xeon Gold 5318Y CPU.
Each contour level within the plots represents a mean GFLOPS/core value that is averaged across tensor sizes.

Every contour plot contains all applicable TTM cases listed in Table \ref{tab:mapping_rm_cm}.
The first column of performance values is generated by \ttt{gemm} belonging to the TTM case $3$, except the first element which corresponds to TTM case $2$.
The first row, excluding the first element, is generated by TTM case $6$ function.
TTM case $7$ is covered by the diagonal line of performance values when $q = p$.  
Although Fig. \ref{fig:performance.tlib.contour} suggests that $q>p$ is possible, our profiling program ensures that $q=p$.
TTM case $8$ with multiple \ttt{gemm} calls is represented by the triangular region which is defined by $1<q<p$.


% With <par-loop,seq-gemm>
% Asymmetrically Shaped Tensors
Function \ttt{<par-loop,slice>} runs on average with $34.96$ GFLOPS/core ($1.67$ TFLOPS) with asymmetrically shaped tensors.
With a maximum performance of $57.805$ GFLOPS/core ($2.77$ TFLOPS), it performs on average $89.64$\% faster than \ttt{<par-loop,subtensor>}.
The slowdown with subtensors at $q=p-1$ or $q=p-2$ can be explained by the small loop count of the function that are $2$ and $4$, respectively.
While function \ttt{<par-loop,slice>} is affected by the tensor shapes for dimensions $p=3$ and $p=4$ as well, its performance improves with increasing order due to the increasing loop count.
%The performance drops and their corresponding locations on the performance plots have also been mentioned in \cite{bassoy:2024:ttm}.
%Symmetrically Shaped Tensors
Function \ttt{<par-loop,slice>} achieves on average $17.34$ GFLOPS/core ($832.42$ GFLOPS) if symmetrically shaped tensors are used.
If subtensors are used, function \ttt{<par-loop,subtensor>} achieves a mean throughput of $17.62$ GFLOPS/core ($846.16$ GFLOPS) and is on average $9.89$\% faster than \ttt{<par-loop,slice>}.
The performances of both functions are monotonically decreasing with increasing tensor order, see plots (1.c) and (1.d) in Fig. \ref{fig:performance.tlib.contour}.
%The average performance decrease of both functions can be approximated by a cubic polynomial with the coefficients $-35$, $640$, $-3848$ and $8011$.
%The performance behavior with symmetrically shaped tensors has also been described in \cite{bassoy:2024:ttm}.


% With <seq-loop,par-gemm>
% Asymmetrically Shaped Tensors
Function \ttt{<par-gemm,slice>} averages $36.42$ GFLOPS/core ($1.74$ TFLOPS) and achieves up to $57.91$ GFLOPS/core ($2.77$ TFLOPS) with asymmetrically shaped tensors.
Using subtensors, function \ttt{<par-gemm,subtensor>} exhibits almost identical performance characteristics and is on average $3.42$\% slower than its counterpart with tensor slices.
% Symmetrically Shaped Tensors
For symmetrically shaped tensors, \ttt{<par-gemm>} with subtensors and tensor slices achieve a mean throughput $15.98$ GFLOPS/core ($767.31$ GFLOPS) and $15.43$ GFLOPS/core ($740.67$ GFLOPS), respectively.
However, function \ttt{<par-gemm,subtensor>} is on average $87.74$\% faster than \ttt{<par-gemm,slice>} which is hardly visible due to small performance values around $5$ GFLOPS/core or less whenever $q<p$ and the dimensions are smaller than $256$.
The speedup of the \ttt{<subtensor>} version can be explained by the smaller loop count and slice-matrix multiplications with larger tensor slices.

Our findings indicate that, regardless of the parallelization method employed, subtensors are most effective with symmetrically shaped tensors, whereas tensor slices are preferable with asymmetrically shaped tensors when both the contraction mode and leading dimension are large.



\subsection{Parallelization Methods}
This subsection compares the performance results of the two parallelization methods, \ttt{<par-gemm>} and \ttt{<par-loop>}, as introduced in Section \ref{subsec:parallel.multi-loops} and illustrated in Fig. \ref{fig:performance.tlib.contour}.

% Asymmetrically shaped tensors
With asymmetrically shaped tensors, both \ttt{<par-gemm>} functions with subtensors and tensor slices compute the tensor-matrix product on average with ca. $36$ GFLOPS/core and outperform function \ttt{<par-loop,subtensor>} on average by a factor of $2.31$.
The speedup can be explained by the performance drop of function \ttt{<par-loop,subtensor>} to $3.49$ GFLOPS/core at $q=p-1$ while both versions of \ttt{<par-gemm>} operate around $39$ GFLOPS/core.
Function \ttt{<par-loop,slice>} performs better for reasons explained in the previous subsection.
However, it is on average $30.57$\% slower than function \ttt{<par-gemm,slice>} due to the aforementioned performance drops.

% Symmetrically shaped tensors
In case of symmetrically shaped tensors, \ttt{<par-loop>} with subtensors and tensor slices outperform their corresponding \ttt{<par-gemm>} counterparts by $23.3$\% and $32.9$\%, respectively.
The speedup mostly occurs when $1<q<p$ where the performance gain is a factor of $2.23$.
This performance behavior can be expected as the tensor slice sizes decreases for the eighth case with increasing tensor order causing the parallel slice-matrix multiplication to perform on smaller matrices.
In contrast, \ttt{<par-loop>} can execute small single-threaded slice-matrix multiplications in parallel.

In summary, function \ttt<par-loop,subtensor> with symmetrically shaped tensors performs best.
If the leading and contraction dimensions are large, both versions of function \ttt{<par-gemm>} outperform \ttt{<par-loop>} with any type of slicing. 

\subsection{Loops Over Gemm}
The contour plots in Fig. \ref{fig:performance.tlib.contour} contain performance data that are generated by all applicable TTM cases of each \ttt{ttm} function.
Yet, the presented slicing or parallelization methods only affect the eighth case, while all other TTM cases apply a single multi-threaded \ttt{gemm} with the same configuration.
The following analysis will consider performance values of the eighth case in order to have a more fine grained visualization and discussion of the loops over \ttt{gemm} implementations.
Fig. \ref{fig:performance.tlib.case8} contains cumulative performance distributions of all the proposed algorithms including the functions \ttt{<batched-gemm>} and \ttt{<combined>} for the eighth TTM case only.
Moreover, the experiments have been additionally executed on the AMD EPYC processor and with the column-major ordering of the input matrix as well.

%Explain what cumulate performance distribution means.
The probability $x$ of a point $(x,y)$ of a distribution function for a given algorithm corresponds to the number of test instances for which that algorithm that achieves a throughput of either $y$ or less.
For instance, function \ttt{<batched-gemm>} computes the tensor-matrix product with asymmetrically shaped tensors in $25$\% of the tensor instances with equal to or less than $10$ GFLOPS/core.
%Consequently, distribution functions with a logarithmic growth are favorable while exponential behavior is less desirable.
Please note that the four plots on the right, plots (c) and (d), have a logarithmic y-axis for a better visualization.


\subsubsection{Combined Algorithm and Batched GEMM}
% can you refer to what figure you are referring to ?
This subsection compares the runtime performance of the functions \ttt{<batched-gemm>} and \ttt{<combined>} against those of \ttt{<par-loop>} and \ttt{<par-gemm>} for the eighth TTM case.

Given a row-major matrix ordering, the combined function \ttt{<combined>} achieves on the Intel processor a median throughput of $36.15$ and $4.28$ GFLOPS/core with asymmetrically and symmetrically shaped tensors.
Reaching up to $46.96$ and $45.68$ GFLOPS/core, it is on par with \ttt{<par-gemm,subtensor>} and \ttt{<par-loop,slice>} and outperforms them for some tensor instances.
Note that both functions run significantly slower either with asymmetrically or symmetrically shaped tensors.
The observable superior performance distribution of \ttt{<combined>} can be attributed to the heuristic which switches between \ttt{<par-loop>} and \ttt{<par-gemm>} depending on the inner and outer loop count as explained in section \ref{subsec:parallel.multi-loops}.

Function \ttt{<batched-gemm>} of the BLAS-like extension library has a performance distribution that is akin to the \ttt{<par-loop,subtensor>}.
In case of asymmetrically shaped tensors, all functions except \ttt{<par-loop,subtensor>} outperform \ttt{<batched-gemm>} on average by a factor of $2.57$ and up to a factor $4$ for $2 \leq q\leq5$ with $q+2 \leq p \leq q+5$. %q+2=p and q+5=p 
In contrast, \ttt{<par-loop,subtensor>} and \ttt{<batched-gemm>} show a similar performance behavior in the plot (1c) and (1d) for symmetrically shaped tensors, running on average $3.55$ and $8.38$ times faster than \ttt{<par-gemm>} with subtensors and tensor slices, respectively.
%Function \ttt{<par-loop,slice>} underperforms for $p>3$, i.e. when the tensor dimensions are less than $64$. % <- Don't know if I need this.
%TODO:These observations have also been mentioned in \cite{bassoy:2024:ttm}.

In summary, \ttt{<combined>} performs as fast as, or faster than, \ttt{<par-gemm,subtensor>} and \ttt{<par-loop,slice>}, depending on the tensor shape. 
Conversely, \ttt{<batched-gemm>} underperforms for asymmetrically shaped tensors with large contraction modes and leading dimensions.

\subsubsection{Matrix Formats}
This subsection discusses if the input matrix storage formats have any affect on the runtime performance of the proposed functions.
The cumulative performance distributions in Fig. \ref{fig:performance.tlib.case8} suggest that the storage format of the input matrix has only a minor impact on the performance.
The Euclidean distance between normalized row-major and column-major performance values is around $5$ or less with a maximum dissimilarity of $11.61$ or $16.97$, indicating a moderate similarity between the corresponding row-major and column-major data sets.
Moreover, their respective median values with their first and third quartiles differ by less than $5$\% with three exceptions where the difference of the median values is between $10$\% and $15$\%.
% for function \ttt{combined} with symmetrically shaped tensors on both processors.


\subsubsection{BLAS Libraries}
This subsection compares the performance of functions that use Intel's Math Kernel Library (MKL) on the Intel Xeon Gold 5318Y processor with those that use the AMD Optimizing CPU Libraries (AOCL) on the AMD EPYC 9354 processor. 
Limiting the performance evaluation to the eighth case, MKL-based functions with asymmetrically shaped tensors run on average between $1.48$ and $2.43$ times faster than those with the AOCL.
For symmetrically shaped tensors, MKL-based functions are between $1.93$ and $5.21$ times faster than those with the AOCL.
In general, MKL-based functions achieve a speedup of at least $1.76$ and $1.71$ compared to their AOCL-based counterpart when asymmetrically and symmetrically shaped tensors are used.
%TODO: can you tell at which cases the performance drops?


\subsection{Layout-Oblivious Algorithms}
Fig. \ref{fig:performance.tlib.format} contains four box plots summarizing the performance distribution of the \ttt{<combined>} function using the AOCL and MKL.
%visualizing double-precision performance statics in double-precision TFLOPS of \tf{<gemm\_batch>} (left subfigure) and \tf{<par-loops,seq-gemm>} with subtensors (right subfigure).
Every $k$-th box plot has been computed from benchmark data with symmetrically shaped order-$7$ tensors that has a $k$-order tensor layout.
The $1$-order and $7$-order layout, for instance, are the first-order and last-order storage formats of an order-$7$ tensor.
%Note that function \ttt{<combined>} only calls \ttt{<par-loop,subtensor>}.

The reduced performance of around $1$ and $2$ GFLOPS can be attributed to the fact that contraction and leading dimensions of symmetrically shaped subtensors are at most $48$ and $8$, respectively.
When \ttt{<combined>} is used with MKL, the relative standard deviations (RSD) of its median performances are $2.51$\% and $0.74$\%, with respect to the row-major and column-major formats.
The RSD of its respective interquartile ranges (IQR) are $4.29$\% and $6.9$\%, indicating a similar performance distributions.
Using \ttt{<combined>} with AOCL, the RSD of its median performances for the row-major and column-major formats are $25.62$\% and $20.66$\%, respectively.
The RSD of its respective IQRs are $10.83$\% and $4.31$\%, indicating a similar performance distributions.
%TODO: The median performances and the interquartile ranges are much narrower distributed in this work than in the original paper, as we have used static scheduling for the omp distribution
A similar performance behavior can be observed also for other \ttt{ttm} variants such as \ttt{<par-loop,slice>}.
The runtime results demonstrate that the function performances stay within an acceptable range independent for different $k$-order tensor layouts and show that our proposed algorithms are not designed for a specific tensor layout.

\subsection{Other Approaches}
This subsection compares our best performing algorithm with libraries that do not use the LoG approach.
\tbf{TCL} implements the TTGT approach with a high-perform tensor-transpose library \tbf{HPTT} which is discussed in \cite{springer:2018:design}.
\tbf{TBLIS} (v1.2.0) implements the GETT approach that is akin to BLIS' algorithm design for the matrix multiplication \cite{matthews:2018:high}.
%TODO: add footnote that is required to enable support for zen4 and skx-2 
The tensor extension of \tbf{Eigen} (v3.4.9) is used by the Tensorflow framework.
Library \tbf{LibTorch} (v2.4.0) is the C++ distribution of PyTorch \cite{paszke:2019:pytorch}.
The \tbf{Tucker} library is a parallel C++ software package for large-scale data compression which provides a local and distributed TTM function \cite{ballard:2020:tuckermpi}.
The local version implements the LoG approach and computes the TTM product similar to our function \ttt{<par-gemm,subtensor>}.
\tbf{TLIB} denotes our library which only calls the previously presented algorithm \tf{<combined>}.
All of the following provided performance and comparison values are the median values.
%We will use performance or percentage tuples of the form (TCL, TBLIS, LibTorch, Eigen) where each tuple element denotes the performance or runtime percentage of a particular library.

Fig. \ref{fig:performance.tlib.case8} compares the performance distribution of our implementation with the previously mentioned libraries.
Using MKL on the Intel CPU, our implementation (TLIB) achieves a median performance of $38.21$ GFLOPS/core ($1.83$ TFLOPS) and reaches a maximum performance of $51.65$ GFLOPS/core ($2.47$ TFLOPS) with asymmetrically shaped tensors.
It outperforms the competing libraries for almost every tensor instance within the test set.
The median library performances are up to $29.85$ GFLOPS/core and are thus at least $18.09$\% slower than TLIB.
%, reaching at most $84.68$\% of TLIB's throughputs. %($24.16$, $29.85$, $28.66$, $14.86$) 
In case of symmetrically shaped tensors, TLIB's median performance is $8.99$ GFLOPS/core.
Except for TBLIS, TLIB outperforms other libraries by at least $87.52$\%.
TBLIS computes the product with $9.84$ GFLOPS/core which is only $1.38$\% slower than TLIB.

%on the right plot in Fig. \ref{fig:performance.tlib.case8}
%TLIB's median performance is $8.99$ GFLOPS/core, other libraries achieve a median performances of ($2.70$, $9.84$, $3.52$, $3.80$) GFLOPS/core.
%On average their performances constitute ($44.65$, $98.63$, $53.32$, $31.59$) percent of TLIB's throughputs.

On the AMD CPU, our implementation with AOCL computes TTM with $24.28$ GFLOPS/core ($1.55$ TFLOPS), reaching a maximum performance of $50.18$ GFLOPS/core ($3.21$ TFLOPS) with asymmetrically shaped tensors.
TBLIS reaches $26.81$ GFLOPS/core ($1.71$ TFLOPS) and is slightly faster than TLIB. 
However, TLIB's upper performance quartile with $30.82$ GFLOPS/core is slightly larger.
TLIB outperforms the remaining libraries by at least $58.80$\%.
%that have a median performance of ($8.07$, $16.04$, $11.49$) GFLOPS/core reaching on average ($27.97$, $62.97$, $54.64$) percent TLIB's throughputs.
In case of symmetrically shaped tensors, TLIB has a median performance of $7.52$ GFLOPS/core ($481.39$ GFLOPS).
It outperforms all other libraries by at least $15.38$\%.
We have observed that TCL and LibTorch have a median performance of less than $2$ GFLOPS/core in the $3$rd and $8$th TTM case which is less than $6$\% and $10$\% of TLIB's median performance with asymmetrically and symmetrically shaped tensors, respectively.

In most instances, TLIB is able to outperform the competing libraries across all TTM cases.
However, there are few exceptions.
On the AMD CPU, TBLIS reaches $101$\% of TLIB's performance for the $6$th TTM case and LibTorch performs as fast as TLIB for the $7$th TTM case for asymmetrically shaped tensors.
One unexpected finding is that LibTorch achieves $96$\% of TLIB's performance with asymmetrically shaped tensors and only $28$\% in case of symmetrically shaped tensors.
%A similar runtime ratio can be observed with asymmetrically shaped tensors on the Intel Xeon Gold 5318Y.
%The competing libraries run on average with less than or equal to $85$\% of TLIB's average performance for all TTM cases independent of the tensor shape with few exceptions.
On the Intel CPU, LibTorch is on average $9.63$\% faster than TLIB in the $7$th TTM case.
The TCL library runs on average as fast as TLIB in the $6$th and $7$th TTM cases .
The performances of TLIB and TBLIS are in the $8$th TTM case almost on par, TLIB running about $7.86$\% faster.
In case of symmetrically shaped tensors, all libraries except Eigen outperform TLIB by about $13$\%, $42$\% and $65$\% in the $7$th TTM case.
TBLIS and TLIB perform equally well in the $8$th TTM case, while other libraries only reach on average $30$\% of TLIB's performance.

\subsection{Summary}
* evaluated the impact of selecting subtensors and tensor slices. 

* evaluated the impact of different parallelization methods, selecting multi-threaded gemm with sequential 

\begin{table*}[h]
\centering
\footnotesize
\begin{tabular}{lccc c}
\toprule
Library    & \multicolumn{3}{c}{Performance [GFLOPS/core]} & Speedup [\%] \\ 
\cmidrule(lr){2-4} \cmidrule(lr){5-5}
& Min   & Median  & Max & Median \\ 
\midrule
TLIB       & $\mathbf{9.39}$  & $\mathbf{38.42}$   & $\mathbf{57.87}$ &  - \\
TCL        & $0.98$  & $24.16$   & $56.34$ & $17.98$     \\
TBLIS      & $8.33$  & $29.85$   & $47.28$ & $23.96$     \\
LibTorch   & $1.05$  & $28.68$   & $46.56$ & $28.21$     \\
Eigen      & $5.85$  & $14.89$   & $15.67$ & $170.77$     \\
\midrule
TLIB       & $0.14$  & $8.99$    & $58.14$ &  - \\
TCL        & $0.04$  & $2.71$    & $56.63$ & $123.92$     \\
TBLIS      & $\mathbf{1.11}$  & $\mathbf{9.84}$  & $45.03$ & $1.38$     \\
LibTorch   & $0.07$  & $3.52$    & $\mathbf{62.20}$ & $87.52$     \\
Eigen      & $0.21$  & $3.80$    & $16.06$ & $216.69$     \\
\bottomrule
\end{tabular}
\hspace{5em}
\begin{tabular}{lccc c}
\toprule
Library    & \multicolumn{3}{c}{Performance [GFLOPS/core]} & Speedup [\%] \\ 
\cmidrule(lr){2-4} \cmidrule(lr){5-5}
& Min   & Median  & Max  & Median \\ 
\midrule
TLIB       & $2.71$  & $24.28$   & $\mathbf{50.18}$ &   -   \\
TCL        & $0.61$  & $8.08$    & $41.82$ & $257.58$     \\
TBLIS      & $\mathbf{9.06}$  & $\mathbf{26.81}$    & $43.83$ & $6.18$     \\
LibTorch   & $0.63$  & $16.04$   & $50.84$ & $58.84$     \\
Eigen      & $4.06$  & $11.49$   & $35.08$ & $83.05$     \\
\midrule
TLIB       & $0.02$  & $\mathbf{7.52}$  & $\mathbf{54.16}$ &  -  \\
TCL        & $0.03$  & $2.03$    & $42.47$ & $122.45$     \\
TBLIS      & $\mathbf{0.39}$  & $6.19$    & $41.11$ & $15.38$     \\
LibTorch   & $0.05$  & $2.64$    & $46.65$ & $74.37$     \\
Eigen      & $0.10$  & $5.58$    & $36.76$ & $43.45$     \\
\bottomrule
\end{tabular}
\caption%
{%
\footnotesize
The table presents the minimum, median, and maximum performance values in GFLOPS/core alongside the median speedup of TLIB compared to other libraries.
The tests were conducted on an Intel Xeon Gold 5318Y CPU (left) and an AMD EPYC 9354 CPU (right). 
The performance values on the upper and lower rows of one table were evaluated using asymmetrically and symmetrically shaped tensors, respectively. 
}
\label{tab:comparison}
\end{table*}
