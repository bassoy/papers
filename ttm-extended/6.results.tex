\section{Results and Discussion}
\label{sec:results}

\begin{comment}
\begin{table*}[t]
\caption%
{%
\footnotesize
Combinable and non-combinable function tags for the eighth TTM case.
%Function tag \tf{combined} and \tf{gemm-batch} are not combinable with the other tags.
}
\centering
\footnotesize
\begin{tabular}{ll}% 
\toprule
Tag & Explanation \\
\midrule
\tf{<par-gemm> }  & \tf{ttm} sequentially executes multi-threaded gemm (section 4.4.1) \\
\tf{<par-loop> }  & \tf{ttm} executes single-threaded \tf{gemm}s in parallel (section 4.4.2) \\
\tf{<slice>}      & \tf{ttm} executes single- or multi-threaded gemm operates with tensor slices \\
\tf{<subtensor>}  & \tf{ttm} executes single- or multi-threaded gemm operates with subtensors  \\  
\tf{<combined>}   & \tf{ttm} calls one of the above functions w.r.t processor core count and tensor dimensions (section 4.4.3) \\
\tf{<gemm-batch>} & \tf{ttm} calls function \tf{gemm\_batch} function of the MKL (section 4.4.4) \\
\bottomrule
\end{tabular}
%\vspace{0.2cm}
\label{tab:function_tags}
\end{table*}
\end{comment}


\begin{figure*}[t]
\input{fig.performance.tlib.contour-mkl-parloop}
\input{fig.performance.tlib.contour-mkl-pargemm} 
\caption{%
\footnotesize %
Performance contour plots in double-precision GFLOPS/core of the proposed TTM algorithms \tf{<par-loop>} and \tf{<par-gemm>} with varying tensor orders $p$ and contraction modes $q$. 
The top row of maps (1x) depict measurements of the \tf{<par-loop>} versions while the bottom row of maps with number (2x) contain measurements of the \tf{<par-gemm>} versions.
Tensors are asymmetrically shaped on the left four maps (a,b) and symmetrically shaped on the right four maps (c,d).
Tensor $\mubA$ and $\mubC$ have the first-order while matrix $\mbB$ has the row-major ordering.
All functions have been measured on an Intel Xeon Gold 5318Y.
\label{fig:performance.tlib.contour}
}
\end{figure*}


\begin{figure*}[t]
\input{fig.performance.tlib.case8-cdf-mkl}
\input{fig.performance.tlib.case8-cdf-aocl}
\caption{ %
\footnotesize%
Cumulative performance distributions in double-precision GFLOPS/core of the proposed algorithms for the eighth case.
Each distribution belongs to one algorithm:
\tf{<batch-gemm>} (\ref{coord:gemm_batch}), %
\tf{<combined>} (\ref{coord:optimized}),
\tf{<par-gemm>}\tf{<slice>} (\ref{coord:seq_loops_par_gemm_slice}) and
\tf{<par-loop>}\tf{<slice>} (\ref{coord:par_loops_seq_gemm_slice}) using tensor slices,
\tf{<par-gemm>}\tf{<subtensor>} (\ref{coord:seq_loops_par_gemm_subtensor}) and
\tf{<par-loop>}\tf{<subtensor>} (\ref{coord:par_loops_seq_gemm_subtensor}) using subtensors.
The top row of maps (1x) depict measurements performed on an Intel Xeon Gold 5318Y with the MKL while the bottom row of maps with number (2x) contain measurements performed on an AMD EPYC 9354 with the AOCL.
Tensors are asymmetrically shaped in (a) and (b) and symmetrically shaped in (c) and (d).
Input matrix has the row-major ordering (rm) in (a) and (c) and column-major ordering (cm) in (b) and (d).
}
\label{fig:performance.tlib.case8}
\end{figure*}


\begin{figure*}[t]
\input{fig.performance.tlib.format.mkl}
\input{fig.performance.tlib.format.aocl}
%\input{fig.performance.tlib.format}
\caption{ %
\footnotesize%
Box plots visualizing performance statics in double-precision GFLOPS/core of the function with row-major (left) or column-major matrices (right).
Box plot number $k$ denotes the $k$-order tensor layout of symmetrically shaped tensors with order $7$.
% where the $1$-order and $7$-order layouts are the first- and last-order storage formats, respectively.
}
\label{fig:performance.tlib.format}
\end{figure*}

\begin{figure*}[t]
\input{fig.performance.comparison-mkl}
\input{fig.performance.comparison-aocl} 
\caption{ %
\footnotesize%
Cumulative performance distributions of TTM implementations in double-precision GFLOPS/core.
Each distribution corresponds to a library:
\textbf{TLIB}[ours] (\ref{coord:nonsymmetric.tlib.slice}), %
\textbf{TCL} (\ref{coord:nonsymmetric.tcl}), %
\textbf{TBLIS} (\ref{coord:nonsymmetric.tblis}), %
\textbf{LibTorch} (\ref{coord:nonsymmetric.libtorch}), %
\textbf{Eigen} (\ref{coord:nonsymmetric.eigen}).
Libraries have been tested with asymmetrically-shaped (left plot) and symmetrically-shaped tensors (right plot).
}
\label{fig:performance.comparison}
\end{figure*}


\subsection{Slicing Methods}
This section analyzes the performance of the two proposed slicing methods \ttt{<slice>} and \ttt{<subtensor>} that have been discussed in section \ref{subsec:parallel.multi-loops}.
%Note that this analysis is equal to minimizing or maximizing the input parameter $M_C$.
Figure \ref{fig:performance.tlib.contour} contains eight performance contour plots of four \ttt{ttm} functions \ttt{<par-loop>} and \ttt{<par-gemm>} that either compute the slice-matrix product with subtensors \ttt{<subtensor>} or tensor slices \ttt{<slice>}.
Each contour level within the plots represents a mean GFLOPS/core value that is averaged across tensor sizes.

Every contour plot contains all applicable TTM cases listed in Table \ref{tab:mapping_rm_cm}.
The first column of performance values is generated by \ttt{gemm} belonging to the TTM case $3$, except the first element which corresponds to TTM case $2$.
The first row, excluding the first element, is generated by TTM case $6$ function.
TTM case $7$ is covered by the diagonal line of performance values when $q = p$.  
Although Figure \ref{fig:performance.tlib.contour} suggests that $q>p$ is possible, our profiling program ensures that $q=p$.
TTM case $8$ with multiple \ttt{gemm} calls is represented by the triangular region which is defined by $1<q<p$.


% With <par-loop,seq-gemm>
% Asymmetrically Shaped Tensors
Function \ttt{<par-loop>} with \ttt{<slice>} runs on average with $34.96$ GFLOPS/core ($1.67$ TFLOPS) with asymmetrically shaped tensors.
With a maximum performance of $57.805$ GFLOPS/core ($2.77$ TFLOPS), it performs on average $89.64$\% faster than function \ttt{<par-loop>} with \ttt{<subtensor>}.
The slowdown with subtensors at $q=p-1$ or $q=p-2$ can be explained by the small loop count of the function that are $2$ and $4$, respectively.
While function \ttt{<par-loop>} with tensor slices is affected by the tensor shapes for dimensions $p=3$ and $p=4$ as well, its performance improves with increasing order due to the increasing loop count.
The performance drops and their corresponding locations on the performance plots have also been mentioned in \cite{bassoy:2024:ttm}.


%Symmetrically Shaped Tensors
Function \ttt{<par-loop>} with tensor slices achieves on average $17.34$ GFLOPS/core ($832.42$ GFLOPS) with symmetrically shaped tensors.
In this case, \ttt{<par-loop>} with subtensors achieves a mean throughput of $17.62$ GFLOPS/core ($846.16$ GFLOPS) and is on average $9.89$\% faster than the \ttt{<slice>} version.
The performances of both functions are monotonically decreasing with increasing tensor order, see plots (1.c) and (1.d) in Figure \ref{fig:performance.tlib.contour}.
The average performance decrease of both functions can be approximated by a cubic polynomial with the coefficients $-35$, $640$, $-3848$ and $8011$.
The decreasing performance behavior for symmetrically shaped tensors has also been described in \cite{bassoy:2024:ttm}.

% With <seq-loop,par-gemm>
% Asymmetrically Shaped Tensors
Function \ttt{<par-gemm>} with tensor slices averages $36.42$ GFLOPS/core ($1.74$ TFLOPS) and achieves up to $57.91$ GFLOPS/core ($2.77$ TFLOPS) with asymmetrically shaped tensors.
With subtensors, function \ttt{<par-gemm>} exhibits almost identical performance characteristics and is on average only $3.42$\% slower than its counterpart with tensor slices.

% Symmetrically Shaped Tensors
For symmetrically shaped tensors, \ttt{<par-gemm>} with subtensors and tensor slices achieve a mean throughput $15.98$ GFLOPS/core ($767.31$ GFLOPS) and $15.43$ GFLOPS/core ($740.67$ GFLOPS), respectively.
However, function \ttt{<par-gemm>} with \ttt{<subtensor>} is on average $87.74$\% faster than the \ttt{slice} which is hardly visible due to small performance values around $5$ GFLOPS/core or less whenever $q<p$ and the dimensions are smaller than $256$.
The speedup of the \ttt{<subtensor>} version can be explained by the smaller loop count and slice-matrix multiplications with larger tensor slices.

%TODO: we could say something about the peak performance and how much is achievable and how much is achieved.

\subsection{Parallelization Methods}
This section discusses the performance results of the two parallelization methods \ttt{<par-gemm>} and \ttt{<par-loop>} using the same Figure \ref{fig:performance.tlib.contour}.

% Asymmetrically shaped tensors
With asymmetrically shaped tensors, both \ttt{<par-gemm>} functions with subtensors and tensor slices compute the tensor-matrix product on average with $36$ GFLOPS/core and outperform function \ttt{<par-loop>} with \ttt{<subtensor>} on average by a factor of $2.31$.
The speedup can be explained by the performance drop of function \ttt{<par-loop>}\allowbreak\ttt{<subtensor>} to $3.49$ GFLOPS/core at $q=p-1$ while both \ttt{<par-gemm>} functions operate around $39$ GFLOPS/core.
Function \ttt{<par-loop>} with tensor slices performs better for reasons explained in the previous subsection.
It is on average $30.57$\% slower than its \ttt{<par-gemm>} version due to the aforementioned performance drops.
%TODO: This is different.

% Symmetrically shaped tensors
In case of symmetrically shaped tensors, \ttt{<par-loop>} with subtensors and tensor slices outperform their corresponding \ttt{<par-gemm>} counterparts by $23.3$\% and $32.9$\%, respectively.
The speedup mostly occurs when $1<q<p$ where the performance gain is a factor of $2.23$.
This performance behavior can be expected as the tensor slice sizes decreases for the eighth case with increasing tensor order causing the parallel slice-matrix multiplication to perform on smaller matrices.
In contrast, \ttt{<par-loop>} can execute small single-threaded slice-matrix multiplications in parallel.

%TODO: Summarizing: * use par-loop with subtensors on symmetrically shaped tensors where the leading and contraction dimensions are relatively small. * use par-gemm with tensor slices or subtensors if your leading and contraction dimensions are large compared to the other dimensions.

\subsection{Loops Over Gemm}
The contour plots in Figure \ref{fig:performance.tlib.contour} contain performance data that are generated by all applicable TTM cases of each \ttt{ttm} function.
Yet, the presented slicing or parallelization methods only affect the eighth case, while all other TTM cases apply a single multi-threaded \ttt{gemm}.
The following analysis will consider performance values of the eighth case in order to have a more fine grained visualization and discussion of the loops over \ttt{gemm} implementations.
Figure \ref{fig:performance.tlib.case8} contains cumulative performance distributions of all the proposed algorithms including the  \ttt{<mkl-batch-gemm>} and \ttt{<combined>} functions for case 8 only.
Moreover, the experiments have been additionally executed on the AMD EPYC processor and with the column-major ordering of the input matrix as well.

%Explain what cumulate performance distribution means.
The probability $x$ of a point $(x,y)$ of a distribution function for a given algorithm corresponds to the number of test instances for which that algorithm that achieves a throughput of either $y$ or less.
For instance, function \ttt{<mkl-batch-gemm>} computes the tensor-matrix product with asymmetrically shaped tensors in $25$\% of the tensor instances with equal to or less than $10$ GFLOPS/core.
%Consequently, distribution functions with a logarithmic growth are favorable while exponential behavior is less desirable.
Please note that the four plots on the right, plots (c) and (d), have a logarithmic y-axis for a better visualization.


\subsubsection{Combined Algorithm and Batched GEMM}
Given a row-major matrix ordering, the combined function \ttt{<combined>} achieves on the Intel processor a median throughput of $36.15$ and $4.28$ GFLOPS/core with asymmetrically and symmetrically shaped tensors.
Reaching up to $46.96$ and $45.68$ GFLOPS/core, it is on par with \ttt{<par-gemm>} with subtensors and \ttt{<par-loop>} with tensor slices and outperforms them for some tensor instances.
Note that both functions run significantly slower either with asymmetrically or symmetrically shaped tensors.
The observable superior performance distribution of \ttt{<combined>} can be attributed to the heuristic which switches between \ttt{<par-loop>} and \ttt{<par-gemm>} depending on the inner and outer loop count.

Function \ttt{<mkl-batch-gemm>} of the BLAS-like extension library has a performance distribution that is akin to the \ttt{<par-loop>} with subtensors.
In case of asymmetrically shaped tensors, all functions except \ttt{<par-loop>} with subtensors outperform \ttt{<mkl-batch-gemm>} on average by a factor of $2.57$ and up to a factor $4$ for $2 \leq q\leq5$ with $q+2 \leq p \leq q+5$. %q+2=p and q+5=p 
In contrast, \ttt{<par-loop>} with subtensors and \ttt{<mkl-batch-gemm>} show a similar performance behavior in the plot (1c) and (1d) for symmetrically shaped tensors, running on average $3.55$ and $8.38$ times faster than \ttt{<par-gemm>} with subtensors and tensor slices, respectively.
Function \ttt{<par-loop>} with tensor slices underperforms for $p>3$, i.e. when the tensor dimensions are less than $64$.
These observations have also been mentioned in \cite{bassoy:2024:ttm}.

%We have also observed no performance improvement if \tf{par-gemm} was used with \tf{par-loop} which is why their distribution functions are not shown in Figure \ref{fig:performance.tlib.case8}.
%TODO: that could be taken up in summary or discussion section.
%Moreover, in most cases the \tf{par-gemm} implementations are independent of the tensor shape slower than \tf{par-loops}, even for smaller tensor slices.
%TODO: the above sentence is not true and this can also be seen the original paper


\subsubsection{Matrix Formats}
%how does the matrix-ordering affect the runtime?
The cumulative performance distributions in Figure \ref{fig:performance.tlib.case8} suggest that the storage format of the input matrix has only a minor impact on the performance.
The Euclidean distance between normalized row-major and column-major performance values is around $5$ or less with a maximum dissimilarity of $11.61$ or $16.97$, indicating a moderate similarity between the corresponding row-major and column-major data sets.
Moreover, their respective median values with their first and third quartiles differ by less than $5$\% with three exceptions where the difference of the median values is between $10$\% and $15$\%.
% for function \ttt{combined} with symmetrically shaped tensors on both processors.


\subsubsection{BLAS Libraries}
This subsection compares the performance of functions that use Intel's Math Kernel Library (MKL) on the Intel Xeon Gold 5318Y processor with those that use the AMD Optimizing CPU Libraries (AOCL) on the AMD EPYC 9354 processor. 
Limiting the performance evaluation to the eighth case, MKL-based functions with asymmetrically shaped tensors run on average between $1.48$ and $2.43$ times faster than those with the AOCL.
For symmetrically shaped tensors, MKL-based functions are between $1.93$ and $5.21$ times faster than those with the AOCL.
In general, MKL-based functions achieve a speedup of at least $1.76$ and $1.71$ compared to their AOCL-based counterpart when asymmetrically and symmetrically shaped tensors are used.
%TODO: can you tell at which cases the performance drops?


\subsection{Layout-Oblivious Algorithms}
Figure \ref{fig:performance.tlib.format} contains four subfigures with box plots summarizing the performance distribution of the \ttt{<combined>} function using the AOCL and MKL.
%visualizing double-precision performance statics in double-precision TFLOPS of \tf{<gemm\_batch>} (left subfigure) and \tf{<par-loops,seq-gemm>} with subtensors (right subfigure).
Every $k$th box plot has been computed from benchmark data with symmetrically shaped order-$7$ tensors that has a $k$-order tensor layout.
The $1$-order and $7$-order layout, for instance, are the first-order and last-order storage formats of an order-$7$ tensor\footnote{The $k$-order tensor layout definition is given in section \ref{sec:preliminaries:layout}}.
Note that \ttt{<combined>} only calls \ttt{<par-loop>} with subtensors.

The reduced performance of around $1$ and $2$ GFLOPS can be attributed to the fact that contraction and leading dimensions of symmetrically shaped subtensors are at most $48$ and $8$, respectively.
When \ttt{<combined>} is used with MKL, the relative standard deviations (RSD) of its median performances are $2.51$\% and $0.74$\%, with respect to the row-major and column-major formats.
The RSD of its respective interquartile ranges (IQR) are $4.29$\% and $6.9$\%, indicating a similar performance distributions.
Using \ttt{<combined>} with AOCL, the RSD of its median performances for the row-major and column-major formats are $25.62$\% and $20.66$\%, respectively.
The RSD of its respective IQRs are $10.83$\% and $4.31$\%, indicating a similar performance distributions.
%TODO: The median performances and the interquartile ranges are much narrower distributed in this work than in the original paper, as we have used static scheduling for the omp distribution

A similar performance behavior can be observed also for other \ttt{ttm} variants such as \ttt{par-loop} with tensor slices or \ttt{par-gemm}.
The runtime results demonstrate that the function performances stay within an acceptable range independent for different $k$-order tensor layouts and show that our proposed algorithms are not designed for a specific tensor layout.

\subsection{Other Approaches}
This subsection compares our best performing algorithm with libraries that do not use the LoG approach.
\tbf{TCL} implements the TTGT approach with a high-perform tensor-transpose library \tbf{HPTT} which is discussed in \cite{springer:2018:design}.
\tbf{TBLIS} (v1.2.0) implements the GETT approach that is akin to BLIS' algorithm design for the matrix multiplication \cite{matthews:2018:high}.
%TODO: add footnote that is required to enable support for zen4 and skx-2 
The tensor extension of \tbf{Eigen} (v3.4.9) is used by the Tensorflow framework.
Library \tbf{LibTorch} (v2.4.0) is the \tf{C++} distribution of PyTorch \cite{paszke:2019:pytorch}.
\tbf{TLIB} denotes our library which only calls the previously presented algorithm \tf{<combined>}.
We will use performance or percentage tuples of the form (TCL, TBLIS, LibTorch, Eigen) where each tuple element denotes the performance or runtime percentage of a particular library.


Figure \ref{fig:performance.tlib.case8} compares the performance distribution of our implementation with the previously mentioned libraries.
Using MKL on the Intel CPU, our implementation (TLIB) achieves a median performance of $38.21$ GFLOPS/core ($1.83$ TFLOPS) and reaches a maximum performance of $51.65$ GFLOPS/core ($2.47$ TFLOPS) with asymmetrically shaped tensors.
It outperforms the competing libraries for almost every tensor instance within the test set.
The median library performances are ($24.16$, $29.85$, $28.66$, $14.86$) GFLOPS/core reaching on average ($84.68$, $80.61$, $78.00$, $36.94$) percent of TLIB's throughputs.
In case of symmetrically shaped tensors other libraries on the right plot in Figure \ref{fig:performance.tlib.case8} run at least 2 times slower than TLIB except for TBLIS.
TLIB's median performance is $8.99$ GFLOPS/core, other libraries achieve a median performances of ($2.70$, $9.84$, $3.52$, $3.80$) GFLOPS/core.
On average their performances constitute ($44.65$, $98.63$, $53.32$, $31.59$) percent of TLIB's throughputs.

On the AMD CPU, our implementation with AOCL computes the tensor-times-matrix product on average with $24.28$ GFLOPS/core ($1.55$ TFLOPS) and reaches a maximum performance of $45.84$ GFLOPS/core ($2.93$ TFLOPS) with asymmetrically shaped tensors.
TBLIS reaches $26.81$ GFLOPS/core ($1.71$ TFLOPS) and is slightly faster than TLIB. 
However, TLIB's upper performance quartile with $30.82$ GFLOPS/core is slightly larger. 
TLIB outperforms other competing libraries that have a median performance of ($8.07$, $16.04$, $11.49$) GFLOPS/core reaching on average ($27.97$, $62.97$, $54.64$) percent TLIB's throughputs.
In case of symmetrically shaped tensors, TLIB outperforms all other libraries with $7.52$ GFLOPS/core ($481.39$ GFLOPS) and a maximum performance of $47.78$ GFLOPS/core ($3.05$ TFLOPS).
Other libraries perform with ($2.03$, $6.18$, $2.64$, $5.58$) GFLOPS/core and reach ($44.94$, $86.67$, $57.33$, $69.72$) percent of TLIB's throughputs.
We have observed that TCL and LibTorch have a median performance of less than $2$ GFLOPS/core in the $3$rd and $8$th TTM case which is less than $6$\% and $10$\% of TLIB's median performance with asymmetrically and symmetrically shaped tensors, respectively.

While all libraries run on average $25$\% slower than TLIB across all TTM cases, there are few exceptions.
On the AMD CPU, TBLIS reaches $101$\% of TLIB's performance for the $6$th TTM case and LibTorch performs as fast as TLIB for the $7$th TTM case for asymmetrically shaped tensors.
One unexpected finding is that LibTorch achieves $96$\% of TLIB's performance with asymmetrically shaped tensors and only $28$\% in case of symmetrically shaped tensors.

%A similar runtime ratio can be observed with asymmetrically shaped tensors on the Intel Xeon Gold 5318Y.
%The competing libraries run on average with less than or equal to $85$\% of TLIB's average performance for all TTM cases independent of the tensor shape with few exceptions.
On the Intel CPU, LibTorch is on average $9.63$\% faster than TLIB in the $7$th TTM case.
The TCL library runs on average as fast as TLIB in the $6$th and $7$th TTM cases .
The performances of TLIB and TBLIS are in the $8$th TTM case almost on par, TLIB running about $7.86$\% faster.
In case of symmetrically shaped tensors, all libraries except Eigen outperform TLIB by about $13$\%, $42$\% and $65$\% in the $7$th TTM case.
TBLIS and TLIB perform equally well in the $8$th TTM case, while other libraries only reach on average $30$\% of TLIB's performance.


%All libraries operate with $801.68$ or less GFLOPS for the cases 2 and 3 which is almost half of tlib's performance with $1579$ GFLOPS.
%TODO: This is a bit less 
%The median performance and the interquartile range of tblis and tlib for the cases 6 and 7 are almost the same.
%Their respective median GFLOPS are $255.23$ and $263.94$ for the sixth case and $121.17$ and $144.27$ for the seventh case.
%This explains the similar performance distributions when their performance is less than $400$ GFLOPS.
%Libtorch and eigen compute the tensor-matrix product, in median, with $17.11$ and $9.64$ Gfops/s, respectively.
%Our library tlib has a median performance of $102.11$ GFLOPS and outperforms tblis with $79.35$ GFLOPS for the eighth case.


%Comparing \tf{TCL} performance, \tf{TLIB-SB-PN} achieves an average speedup of $6$x and more than $8$x for $42$\% of the test cases with asymmetrically shaped tensors and executes on average $5$x faster with symmetrically shaped tensors.
%In comparison with \tf{TBLIS}, \tf{TLIB-SB-PN} computes the tensor-vector product on average $4$x and $3.5$x faster for asymmetrically and symmetrically shaped tensors, respectively.
